{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Deep Q Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import gym, random, pickle, os.path, math, glob\n",
    "import numpy as np\n",
    "\n",
    "from timeit import default_timer as timer\n",
    "from datetime import timedelta\n",
    "from timeit import default_timer as timer\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "\n",
    "import matplotlib\n",
    "%matplotlib inline\n",
    "from IPython.display import clear_output\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from utils.wrappers import *\n",
    "from utils.hyperparameters import Config\n",
    "from utils.plot import plot_reward\n",
    "\n",
    "from agents.BaseAgent import BaseAgent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "config = Config()\n",
    "\n",
    "config.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "#epsilon variables\n",
    "config.epsilon_start    = 1.0\n",
    "config.epsilon_final    = 0.01\n",
    "config.epsilon_decay    = 30000\n",
    "config.epsilon_by_frame = lambda frame_idx: config.epsilon_final + (config.epsilon_start - config.epsilon_final) * math.exp(-1. * frame_idx / config.epsilon_decay)\n",
    "\n",
    "#misc agent variables\n",
    "config.GAMMA = 0.99\n",
    "config.LR    = 1e-4\n",
    "\n",
    "#memory\n",
    "config.TARGET_NET_UPDATE_FREQ = 1000\n",
    "config.EXP_REPLAY_SIZE        = 10000  # 100000\n",
    "config.BATCH_SIZE             = 32\n",
    "\n",
    "#Learning control variables\n",
    "config.LEARN_START = 50  # 10000\n",
    "config.MAX_FRAMES  = 100  # 1000000\n",
    "config.UPDATE_FREQ = 4  # 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Replay Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ExperienceReplayMemory:\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.memory = []\n",
    "\n",
    "    def push(self, transition):\n",
    "        self.memory.append(transition)\n",
    "        if len(self.memory) > self.capacity:\n",
    "            del self.memory[0]\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size), None, None\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Network Declaration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    def __init__(self, input_shape, num_actions):\n",
    "        super(DQN, self).__init__()\n",
    "        \n",
    "        self.input_shape = input_shape\n",
    "        self.num_actions = num_actions\n",
    "\n",
    "        self.conv1 = nn.Conv2d(self.input_shape[0], 32, kernel_size=8, stride=4)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=4, stride=2)\n",
    "        self.conv3 = nn.Conv2d(64, 64, kernel_size=3, stride=1)\n",
    "\n",
    "        self.fc1 = nn.Linear(self.feature_size(), 512)\n",
    "        self.fc2 = nn.Linear(512, self.num_actions)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        return x\n",
    "    \n",
    "    def feature_size(self):\n",
    "        return self.conv3(self.conv2(self.conv1(torch.zeros(1, *self.input_shape)))).view(1, -1).size(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Model(BaseAgent):\n",
    "    def __init__(self, static_policy=False, env=None, config=None, log_dir='/tmp/gym'):\n",
    "        super(Model, self).__init__(config=config, env=env, log_dir=log_dir)\n",
    "        self.device = config.device\n",
    "\n",
    "        self.gamma = config.GAMMA\n",
    "        self.lr = config.LR\n",
    "        self.target_net_update_freq = config.TARGET_NET_UPDATE_FREQ\n",
    "        self.experience_replay_size = config.EXP_REPLAY_SIZE\n",
    "        self.batch_size = config.BATCH_SIZE\n",
    "        self.learn_start = config.LEARN_START\n",
    "        self.update_freq = config.UPDATE_FREQ\n",
    "\n",
    "        self.static_policy = static_policy\n",
    "        self.num_feats = env.observation_space.shape\n",
    "        self.num_actions = env.action_space.n\n",
    "        self.env = env\n",
    "\n",
    "        self.declare_networks()\n",
    "            \n",
    "        self.target_model.load_state_dict(self.model.state_dict())\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=self.lr)\n",
    "        \n",
    "        #move to correct device\n",
    "        self.model = self.model.to(self.device)\n",
    "        self.target_model.to(self.device)\n",
    "\n",
    "        if self.static_policy:\n",
    "            self.model.eval()\n",
    "            self.target_model.eval()\n",
    "        else:\n",
    "            self.model.train()\n",
    "            self.target_model.train()\n",
    "\n",
    "        self.update_count = 0\n",
    "\n",
    "        self.declare_memory()\n",
    "\n",
    "    def declare_networks(self):\n",
    "        self.model = DQN(self.num_feats, self.num_actions)\n",
    "        self.target_model = DQN(self.num_feats, self.num_actions)\n",
    "\n",
    "    def declare_memory(self):\n",
    "        self.memory = ExperienceReplayMemory(self.experience_replay_size)\n",
    "\n",
    "    def append_to_replay(self, s, a, r, s_):\n",
    "        self.memory.push((s, a, r, s_))\n",
    "\n",
    "    def prep_minibatch(self):\n",
    "        # random transition batch is taken from experience replay memory\n",
    "        transitions, indices, weights = self.memory.sample(self.batch_size)\n",
    "        \n",
    "        batch_state, batch_action, batch_reward, batch_next_state = zip(*transitions)\n",
    "\n",
    "        shape = (-1,)+self.num_feats\n",
    "\n",
    "        batch_state = torch.tensor(batch_state, device=self.device, dtype=torch.float).view(shape)\n",
    "        batch_action = torch.tensor(batch_action, device=self.device, dtype=torch.long).squeeze().view(-1, 1)\n",
    "        batch_reward = torch.tensor(batch_reward, device=self.device, dtype=torch.float).squeeze().view(-1, 1)\n",
    "        \n",
    "        non_final_mask = torch.tensor(tuple(map(lambda s: s is not None, batch_next_state)), device=self.device, dtype=torch.uint8)\n",
    "        try: #sometimes all next states are false\n",
    "            non_final_next_states = torch.tensor([s for s in batch_next_state if s is not None], device=self.device, dtype=torch.float).view(shape)\n",
    "            empty_next_state_values = False\n",
    "        except:\n",
    "            non_final_next_states = None\n",
    "            empty_next_state_values = True\n",
    "\n",
    "        return batch_state, batch_action, batch_reward, non_final_next_states, non_final_mask, empty_next_state_values, indices, weights\n",
    "\n",
    "    def compute_loss(self, batch_vars):\n",
    "        batch_state, batch_action, batch_reward, non_final_next_states, non_final_mask, empty_next_state_values, indices, weights = batch_vars\n",
    "\n",
    "        #estimate\n",
    "        current_q_values = self.model(batch_state).gather(1, batch_action)\n",
    "        \n",
    "        #target\n",
    "        with torch.no_grad():\n",
    "            max_next_q_values = torch.zeros(self.batch_size, device=self.device, dtype=torch.float).unsqueeze(dim=1)\n",
    "            if not empty_next_state_values:\n",
    "                max_next_action = self.get_max_next_state_action(non_final_next_states)\n",
    "                max_next_q_values[non_final_mask] = self.target_model(non_final_next_states).gather(1, max_next_action)\n",
    "            expected_q_values = batch_reward + self.gamma*max_next_q_values\n",
    "\n",
    "        diff = (expected_q_values - current_q_values)\n",
    "        loss = self.MSE(diff)\n",
    "        loss = loss.mean()\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def update(self, s, a, r, s_, frame=0):\n",
    "        if self.static_policy:\n",
    "            return None\n",
    "\n",
    "        self.append_to_replay(s, a, r, s_)\n",
    "\n",
    "        if frame < self.learn_start or frame % self.update_freq != 0:\n",
    "            return None\n",
    "\n",
    "        batch_vars = self.prep_minibatch()\n",
    "\n",
    "        loss = self.compute_loss(batch_vars)\n",
    "\n",
    "        # Optimize the model\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        for param in self.model.parameters():\n",
    "            param.grad.data.clamp_(-1, 1)\n",
    "        self.optimizer.step()\n",
    "\n",
    "        self.update_target_model()\n",
    "        self.save_td(loss.item(), frame)\n",
    "        self.save_sigma_param_magnitudes(frame)\n",
    "\n",
    "    def get_action(self, s, eps=0.1):\n",
    "        with torch.no_grad():\n",
    "            if np.random.random() >= eps or self.static_policy:\n",
    "                X = torch.tensor([s], device=self.device, dtype=torch.float)\n",
    "                a = self.model(X).max(1)[1].view(1, 1)\n",
    "                return a.item()\n",
    "            else:\n",
    "                return np.random.randint(0, self.num_actions)\n",
    "\n",
    "    def update_target_model(self):\n",
    "        self.update_count+=1\n",
    "        self.update_count = self.update_count % self.target_net_update_freq\n",
    "        if self.update_count == 0:\n",
    "            self.target_model.load_state_dict(self.model.state_dict())\n",
    "\n",
    "    def get_max_next_state_action(self, next_states):\n",
    "        return self.target_model(next_states).max(dim=1)[1].view(-1, 1)\n",
    "\n",
    "    def finish_nstep(self):\n",
    "        pass\n",
    "\n",
    "    def reset_hx(self):\n",
    "        pass\n",
    "    \n",
    "    def MSE(self, x):\n",
    "        return 0.5 * x.pow(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frame: 100\r\n"
     ]
    }
   ],
   "source": [
    "start=timer()\n",
    "\n",
    "# log_dir = \"/tmp/gym/\"\n",
    "log_dir = \"./log/\"\n",
    "try:\n",
    "    os.makedirs(log_dir)\n",
    "except OSError:\n",
    "    files = glob.glob(os.path.join(log_dir, '*.monitor.csv'))\n",
    "    for f in files:\n",
    "        os.remove(f)\n",
    "\n",
    "env_id = \"PongNoFrameskip-v4\"\n",
    "# env_id = \"AssaultNoFrameskip-v4\"\n",
    "env    = make_atari(env_id)\n",
    "env    = bench.Monitor(env, os.path.join(log_dir, env_id))\n",
    "env    = wrap_deepmind(env, episode_life=True, clip_rewards=True, frame_stack=False, scale=True)\n",
    "env    = WrapPyTorch(env)\n",
    "gpu_id = 1\n",
    "\n",
    "with torch.cuda.device(gpu_id):\n",
    "    model  = Model(env=env, config=config)\n",
    "\n",
    "    episode_reward = 0\n",
    "\n",
    "    observation = env.reset()\n",
    "    for frame_idx in range(1, config.MAX_FRAMES + 1):\n",
    "        epsilon = config.epsilon_by_frame(frame_idx)\n",
    "\n",
    "        action = model.get_action(observation, epsilon)\n",
    "        prev_observation=observation\n",
    "        observation, reward, done, _ = env.step(action)\n",
    "        observation = None if done else observation\n",
    "\n",
    "        model.update(prev_observation, action, reward, observation, frame_idx)\n",
    "        episode_reward += reward\n",
    "\n",
    "        if done:\n",
    "            model.finish_nstep()\n",
    "            model.reset_hx()\n",
    "            observation = env.reset()\n",
    "            model.save_reward(episode_reward)\n",
    "            episode_reward = 0\n",
    "\n",
    "        if frame_idx % 100 == 0:\n",
    "            print('Frame: {}\\r'.format(frame_idx))\n",
    "\n",
    "        if frame_idx % 1000 == 0:\n",
    "            try:\n",
    "                clear_output(True)\n",
    "                plot_reward(log_dir, env_id, 'DQN', config.MAX_FRAMES, bin_size=10, smooth=1, time=timedelta(seconds=int(timer()-start)), ipynb=True)\n",
    "            except IOError:\n",
    "                pass\n",
    "\n",
    "    model.save_w(env_id)\n",
    "    model.save_w()\n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    ""
   ]
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python tf1_10",
   "language": "python",
   "name": "tf1_10"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3.0
 },
 "nbformat": 4,
 "nbformat_minor": 0
}